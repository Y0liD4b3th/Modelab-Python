{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f04408",
   "metadata": {},
   "source": [
    "\n",
    "# Versi√≥n *LIMPIA* para Jupyter/VS Code ‚úÖ\n",
    "\n",
    "Este notebook fue **migrado desde Colab** y qued√≥ **listo para correr localmente**.\n",
    "- Se **eliminaron** celdas/ l√≠neas espec√≠ficas de Colab (montaje de Drive, `files.upload/download`, `apt-get`, imports de `google.colab`).\n",
    "- Se **migraron** rutas de `/content/...` a rutas locales con `BASE_DIR`.\n",
    "- Se agregaron **anotaciones** antes de celdas que fueron modificadas.\n",
    "\n",
    "> Si algo no corre por falta de paquetes, instala lo que aparece en la secci√≥n **Requerimientos (pip)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ea5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuraci√≥n de proyecto local ===\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚¨áÔ∏è EDITA ESTA RUTA si tu proyecto vive en otra carpeta\n",
    "BASE_DIR = Path.cwd()  # Por defecto: carpeta donde est√° el notebook\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f9005",
   "metadata": {},
   "source": [
    "### üìù Anotaciones para la celda siguiente\n",
    "- Se eliminaron importaciones de **google.colab**.\n",
    "- Se elimin√≥ `drive.mount(...)` (usar rutas locales con `BASE_DIR`).\n",
    "- Se elimin√≥ `files.upload/download` (usa rutas locales o widgets en Jupyter).\n",
    "- Se convirtieron rutas `/content/...` a `str(BASE_DIR / '...')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Celda limpia para Jupyter ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Union\n",
    "import warnings\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Para Colab\n",
    "try:\n",
    "# [REMOVIDO] importaciones de google.colab\n",
    "# [REMOVIDO] importaciones de google.colab\n",
    "    COLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COLAB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Google Colab no disponible. Usando modo local.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TTestAnalyzer:\n",
    "    def __init__(self, alpha: float = 0.05):\n",
    "        self.alpha = alpha\n",
    "        self.results = {}\n",
    "        self.uploaded_data = None\n",
    "\n",
    "    def upload_file(self, method: str = 'upload') -> pd.DataFrame:\n",
    "        if method == 'upload' and COLAB_AVAILABLE:\n",
    "            print(\"üìÅ Seleccione su archivo de datos:\")\n",
    "# [REMOVIDO] files.upload/download (usar rutas locales)\n",
    "\n",
    "            if not uploaded:\n",
    "                raise ValueError(\"No se subi√≥ ning√∫n archivo\")\n",
    "\n",
    "            filename = list(uploaded.keys())[0]\n",
    "            file_content = uploaded[filename]\n",
    "\n",
    "            return self._process_file(filename, file_content)\n",
    "\n",
    "        elif method == 'drive' and COLAB_AVAILABLE:\n",
    "            print(\"üîó Montando Google Drive...\")\n",
    "# [REMOVIDO] drive.mount(...)\n",
    "\n",
    "            print(\"\\nüìÇ Algunos archivos en su Drive:\")\n",
    "            try:\n",
    "                drive_path = Path(str(BASE_DIR / r\"\"))\n",
    "                files_found = []\n",
    "\n",
    "                for ext in ['*.csv', '*.xlsx', '*.xls', '*.json']:\n",
    "                    files_found.extend(list(drive_path.rglob(ext))[:3])\n",
    "\n",
    "                for i, file_path in enumerate(files_found[:10], 1):\n",
    "                    print(f\"   {i}. {file_path.name} ({file_path.parent.name})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error listando archivos: {e}\")\n",
    "\n",
    "            file_path = input(\"\\nüìù Ingrese la ruta completa del archivo: \")\n",
    "\n",
    "            if not file_path.startswith(str(BASE_DIR / r\"drive\")):\n",
    "                file_path = str(BASE_DIR / r\"\") + file_path.lstrip('/')\n",
    "\n",
    "            return self._load_from_path(file_path)\n",
    "\n",
    "        elif method == 'local':\n",
    "            file_path = input(\"üìù Ingrese la ruta completa del archivo: \")\n",
    "            return self._load_from_path(file_path)\n",
    "\n",
    "        elif method == 'url':\n",
    "            url = input(\"üåê Ingrese la URL del archivo: \")\n",
    "            try:\n",
    "                if url.endswith('.csv'):\n",
    "                    return pd.read_csv(url)\n",
    "                elif url.endswith(('.xlsx', '.xls')):\n",
    "                    return pd.read_excel(url)\n",
    "                elif url.endswith('.json'):\n",
    "                    return pd.read_json(url)\n",
    "                else:\n",
    "                    return pd.read_csv(url)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error cargando desde URL: {e}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"M√©todo no soportado o Colab no disponible\")\n",
    "\n",
    "    def _process_file(self, filename: str, file_content: bytes) -> pd.DataFrame:\n",
    "        file_ext = Path(filename).suffix.lower()\n",
    "\n",
    "        try:\n",
    "            if file_ext == '.csv':\n",
    "                for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "                    try:\n",
    "                        content_str = file_content.decode(encoding)\n",
    "                        for sep in [',', ';', '\\t', '|']:\n",
    "                            try:\n",
    "                                df = pd.read_csv(io.StringIO(content_str), sep=sep)\n",
    "                                if df.shape[1] > 1:\n",
    "                                    print(f\"‚úÖ Archivo CSV cargado (encoding: {encoding}, separador: '{sep}')\")\n",
    "                                    return df\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                        df = pd.read_csv(io.StringIO(content_str))\n",
    "                        print(f\"‚úÖ Archivo CSV cargado (encoding: {encoding})\")\n",
    "                        return df\n",
    "\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "\n",
    "                raise ValueError(\"No se pudo decodificar el archivo CSV\")\n",
    "\n",
    "            elif file_ext in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(io.BytesIO(file_content))\n",
    "                print(\"‚úÖ Archivo Excel cargado\")\n",
    "                return df\n",
    "\n",
    "            elif file_ext == '.json':\n",
    "                content_str = file_content.decode('utf-8')\n",
    "                data = json.loads(content_str)\n",
    "                df = pd.json_normalize(data) if isinstance(data, list) else pd.DataFrame(data)\n",
    "                print(\"‚úÖ Archivo JSON cargado\")\n",
    "                return df\n",
    "\n",
    "            elif file_ext in ['.txt', '.tsv']:\n",
    "                content_str = file_content.decode('utf-8')\n",
    "                df = pd.read_csv(io.StringIO(content_str), sep='\\t')\n",
    "                print(\"‚úÖ Archivo de texto delimitado cargado\")\n",
    "                return df\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Tipo de archivo no soportado: {file_ext}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error procesando archivo {filename}: {e}\")\n",
    "\n",
    "    def _load_from_path(self, file_path: str) -> pd.DataFrame:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Archivo no encontrado: {file_path}\")\n",
    "\n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "\n",
    "        try:\n",
    "            if file_ext == '.csv':\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                print(\"‚úÖ Archivo CSV cargado desde ruta local\")\n",
    "                return df\n",
    "\n",
    "            elif file_ext in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(file_path)\n",
    "                print(\"‚úÖ Archivo Excel cargado desde ruta local\")\n",
    "                return df\n",
    "\n",
    "            elif file_ext == '.json':\n",
    "                df = pd.read_json(file_path)\n",
    "                print(\"‚úÖ Archivo JSON cargado desde ruta local\")\n",
    "                return df\n",
    "\n",
    "            elif file_ext in ['.txt', '.tsv']:\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "                print(\"‚úÖ Archivo de texto cargado desde ruta local\")\n",
    "                return df\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Tipo de archivo no soportado: {file_ext}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error cargando archivo {file_path}: {e}\")\n",
    "\n",
    "    def validate_data(self, df: pd.DataFrame, show_details: bool = True) -> dict:\n",
    "        report = {\n",
    "            'shape': df.shape,\n",
    "            'columns': list(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'duplicated_rows': df.duplicated().sum(),\n",
    "            'numeric_columns': [],\n",
    "            'non_numeric_columns': [],\n",
    "            'problems': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "\n",
    "        if show_details:\n",
    "            print(\"üîç VALIDACI√ìN DE DATOS\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"üìä Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "            print(f\"üìù Columnas: {', '.join(df.columns)}\")\n",
    "\n",
    "        # Clasificar columnas por tipo\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                report['numeric_columns'].append(col)\n",
    "            else:\n",
    "                report['non_numeric_columns'].append(col)\n",
    "\n",
    "        if show_details:\n",
    "            print(f\"üî¢ Columnas num√©ricas: {len(report['numeric_columns'])}\")\n",
    "            print(f\"üìù Columnas no num√©ricas: {len(report['non_numeric_columns'])}\")\n",
    "\n",
    "        # Verificar valores faltantes REALES\n",
    "        missing_total = df.isnull().sum().sum()\n",
    "        if missing_total > 0:\n",
    "            missing_by_column = df.isnull().sum()\n",
    "            real_missing_problems = False\n",
    "\n",
    "            if df.shape[1] == 2:\n",
    "                col1, col2 = df.columns\n",
    "                missing1 = missing_by_column[col1]\n",
    "                missing2 = missing_by_column[col2]\n",
    "\n",
    "                # Verificar si los faltantes siguen un patr√≥n de muestras independientes\n",
    "                # (valores faltantes al final de las columnas)\n",
    "                if missing1 > 0:\n",
    "                    last_valid_idx1 = df[col1].last_valid_index()\n",
    "                    if last_valid_idx1 is not None:\n",
    "                        # Verificar si todos los valores despu√©s del √∫ltimo v√°lido son NaN\n",
    "                        after_last_valid = df.loc[last_valid_idx1+1:, col1]\n",
    "                        missing_at_end1 = len(after_last_valid) == 0 or after_last_valid.isnull().all()\n",
    "                    else:\n",
    "                        missing_at_end1 = False\n",
    "                else:\n",
    "                    missing_at_end1 = True\n",
    "\n",
    "                if missing2 > 0:\n",
    "                    last_valid_idx2 = df[col2].last_valid_index()\n",
    "                    if last_valid_idx2 is not None:\n",
    "                        after_last_valid = df.loc[last_valid_idx2+1:, col2]\n",
    "                        missing_at_end2 = len(after_last_valid) == 0 or after_last_valid.isnull().all()\n",
    "                    else:\n",
    "                        missing_at_end2 = False\n",
    "                else:\n",
    "                    missing_at_end2 = True\n",
    "\n",
    "                # Solo es problema si los faltantes NO est√°n al final\n",
    "                real_missing_problems = not (missing_at_end1 and missing_at_end2)\n",
    "\n",
    "                if not real_missing_problems and show_details:\n",
    "                    valid1 = len(df) - missing1\n",
    "                    valid2 = len(df) - missing2\n",
    "                    print(f\"\\nüìä Detectadas muestras independientes:\")\n",
    "                    print(f\"   {col1}: {valid1} observaciones\")\n",
    "                    print(f\"   {col2}: {valid2} observaciones\")\n",
    "                    if valid1 != valid2:\n",
    "                        print(\"   ‚úÖ Tama√±os diferentes son normales para muestras independientes\")\n",
    "                    else:\n",
    "                        print(\"   ‚ÑπÔ∏è  Mismo tama√±o: pueden ser independientes o pareadas\")\n",
    "            else:\n",
    "                # Para m√°s de 2 columnas o 1 columna, cualquier faltante es problema\n",
    "                real_missing_problems = True\n",
    "\n",
    "            if real_missing_problems:\n",
    "                report['problems'].append(f\"Valores faltantes problem√°ticos: {missing_total} total\")\n",
    "                report['recommendations'].append(\"Revisar patrones de valores faltantes - pueden indicar problemas de calidad\")\n",
    "\n",
    "                if show_details:\n",
    "                    print(\"\\n‚ö†Ô∏è  VALORES FALTANTES PROBLEM√ÅTICOS:\")\n",
    "                    for col, missing in report['missing_values'].items():\n",
    "                        if missing > 0:\n",
    "                            pct = (missing / len(df)) * 100\n",
    "                            print(f\"   {col}: {missing} ({pct:.1f}%)\")\n",
    "\n",
    "                            # Mostrar d√≥nde est√°n los faltantes\n",
    "                            null_positions = df[df[col].isnull()].index.tolist()[:5]\n",
    "                            print(f\"      Posiciones de algunos faltantes: {null_positions}\")\n",
    "\n",
    "\n",
    "        # Verificar filas duplicadas\n",
    "        if report['duplicated_rows'] > 0:\n",
    "            report['problems'].append(f\"Filas duplicadas: {report['duplicated_rows']}\")\n",
    "            report['recommendations'].append(\"Revisar y posiblemente eliminar filas duplicadas\")\n",
    "\n",
    "            if show_details:\n",
    "                print(f\"\\n‚ö†Ô∏è  Filas duplicadas: {report['duplicated_rows']}\")\n",
    "\n",
    "        # Verificar columnas no num√©ricas\n",
    "        if report['non_numeric_columns']:\n",
    "            for col in report['non_numeric_columns']:\n",
    "                unique_vals = df[col].nunique()\n",
    "                sample_vals = df[col].dropna().unique()[:5]\n",
    "\n",
    "                report['problems'].append(f\"Columna no num√©rica: {col}\")\n",
    "\n",
    "                if show_details:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Columna no num√©rica '{col}':\")\n",
    "                    print(f\"   Valores √∫nicos: {unique_vals}\")\n",
    "                    print(f\"   Muestra: {list(sample_vals)}\")\n",
    "\n",
    "                try:\n",
    "                    numeric_col = pd.to_numeric(df[col], errors='coerce')\n",
    "                    if not numeric_col.isnull().all():\n",
    "                        conversion_success = (numeric_col.notnull().sum() / len(df)) > 0.8\n",
    "                        if conversion_success:\n",
    "                            report['recommendations'].append(f\"Columna '{col}' puede convertirse a num√©rica\")\n",
    "                            if show_details:\n",
    "                                print(f\"   ‚úÖ Puede convertirse a num√©rica ({conversion_success:.1%} √©xito)\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Verificar distribuciones extremas\n",
    "        for col in report['numeric_columns']:\n",
    "            col_data = df[col].dropna()\n",
    "            if len(col_data) > 0:\n",
    "                Q1 = col_data.quantile(0.25)\n",
    "                Q3 = col_data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                outliers = ((col_data < (Q1 - 1.5 * IQR)) | (col_data > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "                if outliers > len(col_data) * 0.1:\n",
    "                    report['problems'].append(f\"Columna '{col}': {outliers} outliers potenciales\")\n",
    "                    report['recommendations'].append(f\"Revisar outliers en columna '{col}'\")\n",
    "\n",
    "                if col_data.std() < 1e-10:\n",
    "                    report['problems'].append(f\"Columna '{col}': varianza muy baja o constante\")\n",
    "                    report['recommendations'].append(f\"Columna '{col}' puede ser constante - verificar utilidad\")\n",
    "\n",
    "        # Resumen final\n",
    "        if show_details:\n",
    "            print(f\"\\nüìã RESUMEN:\")\n",
    "            print(f\"   ‚úÖ Columnas num√©ricas listas: {len(report['numeric_columns'])}\")\n",
    "            print(f\"   ‚ö†Ô∏è  Problemas encontrados: {len(report['problems'])}\")\n",
    "            print(f\"   üí° Recomendaciones: {len(report['recommendations'])}\")\n",
    "\n",
    "            if report['problems']:\n",
    "                print(\"\\nüîß PROBLEMAS DETECTADOS:\")\n",
    "                for i, problem in enumerate(report['problems'], 1):\n",
    "                    print(f\"   {i}. {problem}\")\n",
    "\n",
    "            if report['recommendations']:\n",
    "                print(\"\\nüí° RECOMENDACIONES:\")\n",
    "                for i, rec in enumerate(report['recommendations'], 1):\n",
    "                    print(f\"   {i}. {rec}\")\n",
    "\n",
    "        return report\n",
    "\n",
    "    def clean_data(self, df: pd.DataFrame, validation_report: dict) -> pd.DataFrame:\n",
    "        df_clean = df.copy()\n",
    "        cleaning_actions = []\n",
    "\n",
    "        print(\"\\nüßπ LIMPIEZA AUTOM√ÅTICA DE DATOS\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Convertir columnas no num√©ricas si es posible\n",
    "        for col in validation_report['non_numeric_columns']:\n",
    "            try:\n",
    "                numeric_col = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                if (numeric_col.notnull().sum() / len(df_clean)) > 0.8:\n",
    "                    df_clean[col] = numeric_col\n",
    "                    cleaning_actions.append(f\"Convertida '{col}' a num√©rica\")\n",
    "                    print(f\"‚úÖ Columna '{col}' convertida a num√©rica\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Eliminar filas con todos los valores faltantes\n",
    "        initial_rows = len(df_clean)\n",
    "        df_clean = df_clean.dropna(how='all')\n",
    "        if len(df_clean) < initial_rows:\n",
    "            removed = initial_rows - len(df_clean)\n",
    "            cleaning_actions.append(f\"Eliminadas {removed} filas completamente vac√≠as\")\n",
    "            print(f\"üóëÔ∏è  Eliminadas {removed} filas completamente vac√≠as\")\n",
    "\n",
    "        # Eliminar duplicados\n",
    "        if validation_report['duplicated_rows'] > 0:\n",
    "            df_clean = df_clean.drop_duplicates()\n",
    "            cleaning_actions.append(f\"Eliminadas {validation_report['duplicated_rows']} filas duplicadas\")\n",
    "            print(f\"üóëÔ∏è  Eliminadas {validation_report['duplicated_rows']} filas duplicadas\")\n",
    "\n",
    "        # Filtrar solo columnas num√©ricas para an√°lisis\n",
    "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        if len(numeric_cols) == 0:\n",
    "            raise ValueError(\"No hay columnas num√©ricas para an√°lisis despu√©s de la limpieza\")\n",
    "\n",
    "        df_final = df_clean[numeric_cols].copy()\n",
    "\n",
    "        # Para muestras independientes, NO eliminar filas con valores faltantes\n",
    "        if df_final.shape[1] >= 2:\n",
    "            print(\"üìä Manteniendo estructura para muestras independientes\")\n",
    "            print(\"   (Los valores faltantes por diferencia de tama√±o son normales)\")\n",
    "        else:\n",
    "            initial_rows = len(df_final)\n",
    "            df_final = df_final.dropna()\n",
    "            if len(df_final) < initial_rows:\n",
    "                removed = initial_rows - len(df_final)\n",
    "                cleaning_actions.append(f\"Eliminadas {removed} filas con valores faltantes\")\n",
    "                print(f\"üóëÔ∏è  Eliminadas {removed} filas con valores faltantes\")\n",
    "\n",
    "        print(f\"\\nüìä Datos finales: {df_final.shape[0]} filas √ó {df_final.shape[1]} columnas\")\n",
    "        print(f\"üìù Columnas disponibles: {', '.join(df_final.columns)}\")\n",
    "\n",
    "        if len(df_final) > 0:\n",
    "            print(f\"\\nüìã Muestra de datos limpios:\")\n",
    "            print(df_final.head())\n",
    "\n",
    "            print(f\"\\nüìà Estad√≠sticas b√°sicas:\")\n",
    "            print(df_final.describe())\n",
    "\n",
    "        return df_final\n",
    "\n",
    "    def load_and_validate_file(self, method: str = 'upload') -> pd.DataFrame:\n",
    "        print(\"üéØ CARGA Y VALIDACI√ìN DE ARCHIVO\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        df_raw = self.upload_file(method)\n",
    "        print(f\"‚úÖ Archivo cargado exitosamente\")\n",
    "\n",
    "        print(f\"\\nüëÄ Vista previa de los datos:\")\n",
    "        print(df_raw.head())\n",
    "\n",
    "        validation_report = self.validate_data(df_raw)\n",
    "\n",
    "        if validation_report['problems']:\n",
    "            print(f\"\\n‚ùì Se encontraron {len(validation_report['problems'])} problemas.\")\n",
    "            clean_auto = input(\"¬øDesea aplicar limpieza autom√°tica? (s/n): \").lower().startswith('s')\n",
    "\n",
    "            if clean_auto:\n",
    "                df_clean = self.clean_data(df_raw, validation_report)\n",
    "                self.uploaded_data = df_clean\n",
    "                return df_clean\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Continuando con datos sin limpiar. Algunos an√°lisis pueden fallar.\")\n",
    "                self.uploaded_data = df_raw\n",
    "                return df_raw\n",
    "        else:\n",
    "            print(\"‚úÖ No se encontraron problemas significativos en los datos\")\n",
    "            self.uploaded_data = df_raw\n",
    "            return df_raw\n",
    "\n",
    "    def analyze_from_file(self, method: str = 'upload', mu0: Optional[float] = None,\n",
    "                         column_names: Optional[list] = None,\n",
    "                         paired: Optional[bool] = None,\n",
    "                         equal_var: Optional[bool] = None) -> dict:\n",
    "        df = self.load_and_validate_file(method)\n",
    "\n",
    "        if df.shape[1] > 2:\n",
    "            print(f\"\\nüìä Archivo tiene {df.shape[1]} columnas:\")\n",
    "            for i, col in enumerate(df.columns, 1):\n",
    "                print(f\"   {i}. {col}\")\n",
    "\n",
    "            print(\"\\nSeleccione las columnas para el an√°lisis:\")\n",
    "\n",
    "            if mu0 is not None:\n",
    "                col_idx = int(input(\"N√∫mero de columna para an√°lisis de una muestra: \")) - 1\n",
    "                selected_data = df.iloc[:, [col_idx]]\n",
    "            else:\n",
    "                col1_idx = int(input(\"N√∫mero de primera columna: \")) - 1\n",
    "                col2_idx = int(input(\"N√∫mero de segunda columna: \")) - 1\n",
    "                selected_data = df.iloc[:, [col1_idx, col2_idx]]\n",
    "        else:\n",
    "            selected_data = df\n",
    "\n",
    "        return self.analyze(selected_data, mu0=mu0, column_names=column_names,\n",
    "                          paired=paired, equal_var=equal_var)\n",
    "\n",
    "    def load_data(self, data: Union[pd.DataFrame, np.ndarray, list]) -> pd.DataFrame:\n",
    "        if isinstance(data, (list, np.ndarray)):\n",
    "            data = np.array(data)\n",
    "            if data.ndim == 1:\n",
    "                df = pd.DataFrame({'variable': data})\n",
    "            elif data.ndim == 2:\n",
    "                if data.shape[0] < data.shape[1]:\n",
    "                    data = data.T\n",
    "                df = pd.DataFrame(data, columns=[f'variable_{i+1}' for i in range(data.shape[1])])\n",
    "            else:\n",
    "                raise ValueError(\"Los datos deben ser 1D o 2D\")\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            df = data.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Formato de datos no soportado\")\n",
    "\n",
    "        # NO eliminar filas con NaN autom√°ticamente para preservar muestras independientes\n",
    "        # Solo mostrar informaci√≥n sobre los datos\n",
    "        print(f\"Datos cargados: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "\n",
    "        # Mostrar informaci√≥n sobre valores faltantes sin eliminarlos\n",
    "        missing_info = df.isnull().sum()\n",
    "        if missing_info.sum() > 0:\n",
    "            print(\"Valores faltantes por columna:\")\n",
    "            for col, missing in missing_info.items():\n",
    "                if missing > 0:\n",
    "                    valid_count = df.shape[0] - missing\n",
    "                    print(f\"   {col}: {valid_count} observaciones v√°lidas ({missing} faltantes)\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def check_normality(self, data: np.ndarray, name: str = \"Variable\") -> bool:\n",
    "        if len(data) < 3:\n",
    "            print(f\"‚ö†Ô∏è  {name}: Muy pocas observaciones para prueba de normalidad\")\n",
    "            return True\n",
    "\n",
    "        stat, p_value = stats.shapiro(data)\n",
    "        is_normal = p_value > self.alpha\n",
    "\n",
    "        print(f\"üìä Prueba de normalidad ({name}):\")\n",
    "        print(f\"   Shapiro-Wilk: estad√≠stico={stat:.4f}, p-valor={p_value:.4f}\")\n",
    "        print(f\"   {'‚úÖ Normal' if is_normal else '‚ùå No normal'} (Œ±={self.alpha})\")\n",
    "\n",
    "        return is_normal\n",
    "\n",
    "    def check_equal_variances(self, group1: np.ndarray, group2: np.ndarray) -> bool:\n",
    "        stat, p_value = stats.levene(group1, group2)\n",
    "        equal_vars = p_value > self.alpha\n",
    "\n",
    "        print(f\"üìä Prueba de igualdad de varianzas:\")\n",
    "        print(f\"   Levene: estad√≠stico={stat:.4f}, p-valor={p_value:.4f}\")\n",
    "        print(f\"   {'‚úÖ Varianzas iguales' if equal_vars else '‚ùå Varianzas diferentes'} (Œ±={self.alpha})\")\n",
    "\n",
    "        return equal_vars\n",
    "\n",
    "    def descriptive_stats(self, data: np.ndarray, name: str = \"Variable\") -> dict:\n",
    "        stats_dict = {\n",
    "            'n': len(data),\n",
    "            'media': np.mean(data),\n",
    "            'std': np.std(data, ddof=1),\n",
    "            'mediana': np.median(data),\n",
    "            'min': np.min(data),\n",
    "            'max': np.max(data)\n",
    "        }\n",
    "\n",
    "        print(f\"üìà Estad√≠sticas descriptivas ({name}):\")\n",
    "        print(f\"   n = {stats_dict['n']}\")\n",
    "        print(f\"   Media = {stats_dict['media']:.4f}\")\n",
    "        print(f\"   Desv. Est√°ndar = {stats_dict['std']:.4f}\")\n",
    "        print(f\"   Mediana = {stats_dict['mediana']:.4f}\")\n",
    "        print(f\"   Rango = [{stats_dict['min']:.4f}, {stats_dict['max']:.4f}]\")\n",
    "\n",
    "        return stats_dict\n",
    "\n",
    "    def confidence_interval(self, data: np.ndarray, confidence: float = 0.95) -> Tuple[float, float]:\n",
    "        n = len(data)\n",
    "        mean = np.mean(data)\n",
    "        std_err = stats.sem(data)\n",
    "        t_critical = stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "        margin_error = t_critical * std_err\n",
    "        ci_lower = mean - margin_error\n",
    "        ci_upper = mean + margin_error\n",
    "\n",
    "        return ci_lower, ci_upper\n",
    "\n",
    "    def one_sample_t_test(self, data: np.ndarray, mu0: float,\n",
    "                         alternative: str = 'two-sided') -> dict:\n",
    "        print(\"üî¨ PRUEBA T PARA UNA MUESTRA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        stats_desc = self.descriptive_stats(data, \"Muestra\")\n",
    "\n",
    "        is_normal = self.check_normality(data, \"Muestra\")\n",
    "\n",
    "        t_stat, p_value = stats.ttest_1samp(data, mu0, alternative=alternative)\n",
    "\n",
    "        ci_lower, ci_upper = self.confidence_interval(data, 1 - self.alpha)\n",
    "\n",
    "        results = {\n",
    "            'tipo': 'Una muestra',\n",
    "            'estadistico_t': t_stat,\n",
    "            'p_valor': p_value,\n",
    "            'grados_libertad': len(data) - 1,\n",
    "            'media_muestral': np.mean(data),\n",
    "            'mu_hipotesis': mu0,\n",
    "            'intervalo_confianza': (ci_lower, ci_upper),\n",
    "            'es_significativo': p_value < self.alpha,\n",
    "            'es_normal': is_normal\n",
    "        }\n",
    "\n",
    "        self._print_results(results)\n",
    "        return results\n",
    "\n",
    "    def independent_t_test(self, group1: np.ndarray, group2: np.ndarray,\n",
    "                          equal_var: Optional[bool] = None,\n",
    "                          alternative: str = 'two-sided') -> dict:\n",
    "        print(\"üî¨ PRUEBA T PARA DOS MUESTRAS INDEPENDIENTES\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        stats1 = self.descriptive_stats(group1, \"Grupo 1\")\n",
    "        print()\n",
    "        stats2 = self.descriptive_stats(group2, \"Grupo 2\")\n",
    "        print()\n",
    "\n",
    "        is_normal1 = self.check_normality(group1, \"Grupo 1\")\n",
    "        is_normal2 = self.check_normality(group2, \"Grupo 2\")\n",
    "        print()\n",
    "\n",
    "        if equal_var is None:\n",
    "            equal_var = self.check_equal_variances(group1, group2)\n",
    "            print()\n",
    "\n",
    "        if equal_var:\n",
    "            print(\"üîß Usando prueba t cl√°sica (varianzas iguales)\")\n",
    "        else:\n",
    "            print(\"üîß Usando prueba t de Welch (varianzas desiguales)\")\n",
    "\n",
    "        t_stat, p_value = stats.ttest_ind(group1, group2,\n",
    "                                         equal_var=equal_var,\n",
    "                                         alternative=alternative)\n",
    "\n",
    "        if not equal_var:\n",
    "            s1_sq = np.var(group1, ddof=1)\n",
    "            s2_sq = np.var(group2, ddof=1)\n",
    "            n1, n2 = len(group1), len(group2)\n",
    "\n",
    "            df = (s1_sq/n1 + s2_sq/n2)**2 / ((s1_sq/n1)**2/(n1-1) + (s2_sq/n2)**2/(n2-1))\n",
    "        else:\n",
    "            df = len(group1) + len(group2) - 2\n",
    "\n",
    "        diff_means = np.mean(group1) - np.mean(group2)\n",
    "        if equal_var:\n",
    "            pooled_std = np.sqrt(((len(group1)-1)*np.var(group1, ddof=1) +\n",
    "                                 (len(group2)-1)*np.var(group2, ddof=1)) /\n",
    "                                (len(group1)+len(group2)-2))\n",
    "            se_diff = pooled_std * np.sqrt(1/len(group1) + 1/len(group2))\n",
    "        else:\n",
    "            se_diff = np.sqrt(np.var(group1, ddof=1)/len(group1) +\n",
    "                            np.var(group2, ddof=1)/len(group2))\n",
    "\n",
    "        t_critical = stats.t.ppf((1 + (1-self.alpha)) / 2, df)\n",
    "        margin_error = t_critical * se_diff\n",
    "        ci_lower = diff_means - margin_error\n",
    "        ci_upper = diff_means + margin_error\n",
    "\n",
    "        results = {\n",
    "            'tipo': 'Dos muestras independientes',\n",
    "            'estadistico_t': t_stat,\n",
    "            'p_valor': p_value,\n",
    "            'grados_libertad': df,\n",
    "            'media_grupo1': np.mean(group1),\n",
    "            'media_grupo2': np.mean(group2),\n",
    "            'diferencia_medias': diff_means,\n",
    "            'intervalo_confianza_diferencia': (ci_lower, ci_upper),\n",
    "            'varianzas_iguales': equal_var,\n",
    "            'es_significativo': p_value < self.alpha,\n",
    "            'es_normal_g1': is_normal1,\n",
    "            'es_normal_g2': is_normal2\n",
    "        }\n",
    "\n",
    "        self._print_results(results)\n",
    "        return results\n",
    "\n",
    "    def paired_t_test(self, group1: np.ndarray, group2: np.ndarray,\n",
    "                     alternative: str = 'two-sided') -> dict:\n",
    "        print(\"üî¨ PRUEBA T PARA MUESTRAS PAREADAS\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if len(group1) != len(group2):\n",
    "            raise ValueError(\"Las muestras pareadas deben tener el mismo tama√±o\")\n",
    "\n",
    "        differences = group1 - group2\n",
    "\n",
    "        stats1 = self.descriptive_stats(group1, \"Grupo 1 (Antes)\")\n",
    "        print()\n",
    "        stats2 = self.descriptive_stats(group2, \"Grupo 2 (Despu√©s)\")\n",
    "        print()\n",
    "        stats_diff = self.descriptive_stats(differences, \"Diferencias\")\n",
    "        print()\n",
    "\n",
    "        is_normal = self.check_normality(differences, \"Diferencias\")\n",
    "        print()\n",
    "\n",
    "        t_stat, p_value = stats.ttest_rel(group1, group2, alternative=alternative)\n",
    "\n",
    "        ci_lower, ci_upper = self.confidence_interval(differences, 1 - self.alpha)\n",
    "\n",
    "        results = {\n",
    "            'tipo': 'Muestras pareadas',\n",
    "            'estadistico_t': t_stat,\n",
    "            'p_valor': p_value,\n",
    "            'grados_libertad': len(differences) - 1,\n",
    "            'media_grupo1': np.mean(group1),\n",
    "            'media_grupo2': np.mean(group2),\n",
    "            'diferencia_media': np.mean(differences),\n",
    "            'intervalo_confianza_diferencia': (ci_lower, ci_upper),\n",
    "            'es_significativo': p_value < self.alpha,\n",
    "            'es_normal_diferencias': is_normal\n",
    "        }\n",
    "\n",
    "        self._print_results(results)\n",
    "        return results\n",
    "\n",
    "    def _print_results(self, results: dict):\n",
    "        print(\"üìã RESULTADOS DE LA PRUEBA\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Estad√≠stico t: {results['estadistico_t']:.4f}\")\n",
    "        print(f\"Grados de libertad: {results['grados_libertad']:.1f}\")\n",
    "        print(f\"p-valor: {results['p_valor']:.6f}\")\n",
    "        print(f\"Nivel de significancia: {self.alpha}\")\n",
    "\n",
    "        if 'intervalo_confianza' in results:\n",
    "            ci = results['intervalo_confianza']\n",
    "            print(f\"IC {(1-self.alpha)*100:.0f}% para la media: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "        elif 'intervalo_confianza_diferencia' in results:\n",
    "            ci = results['intervalo_confianza_diferencia']\n",
    "            print(f\"IC {(1-self.alpha)*100:.0f}% para la diferencia: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "\n",
    "        print(\"\\nüéØ INTERPRETACI√ìN:\")\n",
    "        if results['es_significativo']:\n",
    "            print(f\"‚úÖ Resultado SIGNIFICATIVO (p < {self.alpha})\")\n",
    "            print(\"   Se rechaza la hip√≥tesis nula\")\n",
    "        else:\n",
    "            print(f\"‚ùå Resultado NO significativo (p ‚â• {self.alpha})\")\n",
    "            print(\"   No se puede rechazar la hip√≥tesis nula\")\n",
    "\n",
    "    def plot_data(self, data: Union[np.ndarray, tuple], labels: Optional[list] = None):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        if isinstance(data, np.ndarray):\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.hist(data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            plt.title('Histograma')\n",
    "            plt.xlabel('Valor')\n",
    "            plt.ylabel('Frecuencia')\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.boxplot(data)\n",
    "            plt.title('Boxplot')\n",
    "            plt.ylabel('Valor')\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "            plt.title('Q-Q Plot (Normalidad)')\n",
    "\n",
    "        else:\n",
    "            group1, group2 = data\n",
    "            labels = labels or ['Grupo 1', 'Grupo 2']\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.hist([group1, group2], bins=15, alpha=0.7,\n",
    "                    label=labels, color=['skyblue', 'lightcoral'])\n",
    "            plt.title('Histogramas')\n",
    "            plt.xlabel('Valor')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.boxplot([group1, group2], labels=labels)\n",
    "            plt.title('Boxplots')\n",
    "            plt.ylabel('Valor')\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            stats.probplot(group1, dist=\"norm\", plot=plt)\n",
    "            plt.title(f'Q-Q Plot {labels[0]}')\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            stats.probplot(group2, dist=\"norm\", plot=plt)\n",
    "            plt.title(f'Q-Q Plot {labels[1]}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def handle_independent_samples_from_df(self, df: pd.DataFrame, col1_name: str, col2_name: str) -> tuple:\n",
    "        group1 = df[col1_name].dropna().values\n",
    "        group2 = df[col2_name].dropna().values\n",
    "\n",
    "        print(f\"üìä Muestras independientes extra√≠das:\")\n",
    "        print(f\"   {col1_name}: {len(group1)} observaciones\")\n",
    "        print(f\"   {col2_name}: {len(group2)} observaciones\")\n",
    "        print(f\"   ‚úÖ Tama√±os diferentes son v√°lidos para muestras independientes\")\n",
    "\n",
    "        return group1, group2\n",
    "\n",
    "    def analyze(self, data: Union[pd.DataFrame, np.ndarray, list, tuple],\n",
    "                mu0: Optional[float] = None,\n",
    "                column_names: Optional[list] = None,\n",
    "                paired: Optional[bool] = None,\n",
    "                equal_var: Optional[bool] = None) -> dict:\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üßÆ AN√ÅLISIS AUTOM√ÅTICO DE PRUEBAS T\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        if isinstance(data, (tuple, list)) and len(data) == 2:\n",
    "            try:\n",
    "                group1 = np.array(data[0]).flatten()\n",
    "                group2 = np.array(data[1]).flatten()\n",
    "\n",
    "                if not (np.issubdtype(group1.dtype, np.number) and np.issubdtype(group2.dtype, np.number)):\n",
    "                    raise ValueError(\"Los datos deben ser num√©ricos\")\n",
    "\n",
    "                print(f\"üìä Detectadas: DOS variables (arrays separados)\")\n",
    "                print(f\"    Tama√±os: Grupo 1 = {len(group1)}, Grupo 2 = {len(group2)}\")\n",
    "\n",
    "                if paired is None:\n",
    "                    if len(group1) == len(group2):\n",
    "                        paired = input(\"¬øLos datos son pareados? (s/n): \").lower().startswith('s')\n",
    "                    else:\n",
    "                        print(\"    Tama√±os diferentes: Asumiendo muestras independientes\")\n",
    "                        paired = False\n",
    "\n",
    "                if column_names is None:\n",
    "                    labels = ['Grupo 1', 'Grupo 2']\n",
    "                else:\n",
    "                    labels = column_names[:2]\n",
    "\n",
    "                self.plot_data((group1, group2), labels)\n",
    "\n",
    "                if paired:\n",
    "                    if len(group1) != len(group2):\n",
    "                        raise ValueError(\"Las muestras pareadas deben tener el mismo tama√±o\")\n",
    "                    return self.paired_t_test(group1, group2)\n",
    "                else:\n",
    "                    if equal_var is None:\n",
    "                        print(\"\\nPara muestras independientes:\")\n",
    "                        equal_var_input = input(\"¬øLas varianzas son iguales? (s/n/auto): \").lower()\n",
    "\n",
    "                        if equal_var_input.startswith('s'):\n",
    "                            equal_var = True\n",
    "                        elif equal_var_input.startswith('n'):\n",
    "                            equal_var = False\n",
    "                        else:\n",
    "                            equal_var = None\n",
    "\n",
    "                    return self.independent_t_test(group1, group2, equal_var)\n",
    "\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Error procesando tupla: {e}\")\n",
    "                print(\"Intentando procesar como datos regulares...\")\n",
    "\n",
    "        try:\n",
    "            df = self.load_data(data)\n",
    "\n",
    "            if column_names and len(column_names) <= df.shape[1]:\n",
    "                new_columns = column_names[:df.shape[1]]\n",
    "                if len(new_columns) < df.shape[1]:\n",
    "                    for i in range(len(new_columns), df.shape[1]):\n",
    "                        new_columns.append(f'variable_{i+1}')\n",
    "                df.columns = new_columns\n",
    "\n",
    "            if df.shape[1] == 1:\n",
    "                print(\"üìä Detectada: UNA variable\")\n",
    "\n",
    "                if mu0 is None:\n",
    "                    mu0 = float(input(\"Ingrese el valor de referencia (Œº‚ÇÄ) para comparar: \"))\n",
    "\n",
    "                data_array = df.iloc[:, 0].values\n",
    "                self.plot_data(data_array)\n",
    "\n",
    "                return self.one_sample_t_test(data_array, mu0)\n",
    "\n",
    "            elif df.shape[1] == 2:\n",
    "                print(\"üìä Detectadas: DOS variables\")\n",
    "\n",
    "                # Verificar tama√±os de cada grupo primero\n",
    "                group1_size = df.iloc[:, 0].notna().sum()\n",
    "                group2_size = df.iloc[:, 1].notna().sum()\n",
    "\n",
    "                print(f\"   Tama√±os detectados: {df.columns[0]} = {group1_size}, {df.columns[1]} = {group2_size}\")\n",
    "\n",
    "                if paired is None:\n",
    "                    if group1_size == group2_size:\n",
    "                        paired = input(\"Los grupos tienen el mismo tama√±o. ¬øSon datos pareados? (s/n): \").lower().startswith('s')\n",
    "                    else:\n",
    "                        print(\"   ‚úÖ Tama√±os diferentes: Asumiendo muestras INDEPENDIENTES autom√°ticamente\")\n",
    "                        paired = False\n",
    "\n",
    "                if paired:\n",
    "                    # Para datos pareados, necesitamos el mismo tama√±o\n",
    "                    if group1_size != group2_size:\n",
    "                        print(\"‚ö†Ô∏è  Para datos pareados se necesita el mismo n√∫mero de observaciones\")\n",
    "                        print(\"   Considerando solo las filas sin valores faltantes...\")\n",
    "\n",
    "                        df_complete = df.dropna()\n",
    "                        if len(df_complete) == 0:\n",
    "                            raise ValueError(\"No hay filas completas para an√°lisis pareado\")\n",
    "\n",
    "                        group1 = df_complete.iloc[:, 0].values\n",
    "                        group2 = df_complete.iloc[:, 1].values\n",
    "                        print(f\"   Usando {len(group1)} pares completos\")\n",
    "                    else:\n",
    "                        # Si tienen el mismo tama√±o, usar todos los datos disponibles\n",
    "                        group1 = df.iloc[:, 0].dropna().values\n",
    "                        group2 = df.iloc[:, 1].dropna().values\n",
    "\n",
    "                    labels = [df.columns[0], df.columns[1]]\n",
    "                    self.plot_data((group1, group2), labels)\n",
    "\n",
    "                    return self.paired_t_test(group1, group2)\n",
    "\n",
    "                else:\n",
    "                    # Para muestras independientes, extraer cada grupo por separado\n",
    "                    group1, group2 = self.handle_independent_samples_from_df(\n",
    "                        df, df.columns[0], df.columns[1])\n",
    "\n",
    "                    labels = [df.columns[0], df.columns[1]]\n",
    "                    self.plot_data((group1, group2), labels)\n",
    "\n",
    "                    if equal_var is None:\n",
    "                        print(\"\\nPara muestras independientes:\")\n",
    "                        equal_var_input = input(\"¬øLas varianzas son iguales? (s/n/auto): \").lower()\n",
    "\n",
    "                        if equal_var_input.startswith('s'):\n",
    "                            equal_var = True\n",
    "                        elif equal_var_input.startswith('n'):\n",
    "                            equal_var = False\n",
    "                        else:\n",
    "                            equal_var = None\n",
    "\n",
    "                    return self.independent_t_test(group1, group2, equal_var)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Este analizador solo maneja 1 o 2 variables\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en el an√°lisis: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def analizar_desde_archivo(method='upload', mu0=None, column_names=None,\n",
    "                          alpha=0.05, paired=None, equal_var=None):\n",
    "    analyzer = TTestAnalyzer(alpha=alpha)\n",
    "    return analyzer.analyze_from_file(method=method, mu0=mu0,\n",
    "                                    column_names=column_names,\n",
    "                                    paired=paired, equal_var=equal_var)\n",
    "\n",
    "\n",
    "def analizar_t_test(data, mu0=None, column_names=None, alpha=0.05,\n",
    "                   paired=None, equal_var=None):\n",
    "    analyzer = TTestAnalyzer(alpha=alpha)\n",
    "    return analyzer.analyze(data, mu0=mu0, column_names=column_names,\n",
    "                          paired=paired, equal_var=equal_var)\n",
    "\n",
    "\n",
    "def mostrar_metodos_carga():\n",
    "    print(\"üìÅ M√âTODOS DE CARGA DE ARCHIVOS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. 'upload' - Subir archivo manualmente (recomendado)\")\n",
    "    print(\"   - Aparece selector de archivos\")\n",
    "    print(\"   - Soporta: CSV, Excel, JSON, TXT\")\n",
    "    print(\"   - Ejemplo: analizar_desde_archivo('upload')\")\n",
    "\n",
    "    print(\"\\n2. 'drive' - Desde Google Drive\")\n",
    "    print(\"   - Monta autom√°ticamente Google Drive\")\n",
    "    print(\"   - Acceso a todos tus archivos\")\n",
    "    print(\"   - Ejemplo: analizar_desde_archivo('drive')\")\n",
    "\n",
    "    print(\"\\n3. 'local' - Desde sistema local\")\n",
    "    print(\"   - Para uso fuera de Colab\")\n",
    "    print(\"   - Requiere ruta completa del archivo\")\n",
    "    print(\"   - Ejemplo: analizar_desde_archivo('local')\")\n",
    "\n",
    "    print(\"\\n4. 'url' - Desde URL web\")\n",
    "    print(\"   - Carga directa desde internet\")\n",
    "    print(\"   - √ötil para datos p√∫blicos\")\n",
    "    print(\"   - Ejemplo: analizar_desde_archivo('url')\")\n",
    "\n",
    "\n",
    "def crear_datos_ejemplo(tipo='simple'):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if tipo == 'simple':\n",
    "        datos = np.random.normal(100, 15, 30)\n",
    "        return pd.DataFrame({'mediciones': datos})\n",
    "\n",
    "    elif tipo == 'independientes':\n",
    "        control = np.random.normal(85, 12, 25)\n",
    "        tratamiento = np.random.normal(92, 15, 28)\n",
    "        return (control, tratamiento)\n",
    "\n",
    "    elif tipo == 'independientes_df':\n",
    "        control = np.random.normal(85, 12, 25)\n",
    "        tratamiento = np.random.normal(92, 15, 25)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'control': control,\n",
    "            'tratamiento': tratamiento\n",
    "        })\n",
    "\n",
    "    elif tipo == 'pareados':\n",
    "        n = 20\n",
    "        antes = np.random.normal(80, 10, n)\n",
    "        efecto = np.random.normal(8, 5, n)\n",
    "        despues = antes + efecto\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'antes': antes,\n",
    "            'despues': despues\n",
    "        })\n",
    "\n",
    "    elif tipo == 'problemas':\n",
    "        n = 50\n",
    "        datos = {\n",
    "            'numerico_ok': np.random.normal(100, 15, n),\n",
    "            'con_outliers': np.concatenate([\n",
    "                np.random.normal(50, 5, n-5),\n",
    "                [200, 250, -50, -100, 300]\n",
    "            ]),\n",
    "            'con_faltantes': np.random.normal(75, 10, n),\n",
    "            'texto_mezclado': [str(x) if i % 10 == 0 else x\n",
    "                              for i, x in enumerate(np.random.normal(60, 8, n))],\n",
    "            'constante': [42] * n\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(datos)\n",
    "\n",
    "        faltantes_idx = np.random.choice(n, size=10, replace=False)\n",
    "        df.loc[faltantes_idx, 'con_faltantes'] = np.nan\n",
    "\n",
    "        df = pd.concat([df, df.iloc[[0, 5, 10]]], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ ANALIZADOR DE PRUEBAS T DE STUDENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìö Sistema completo para an√°lisis estad√≠stico con t-test\")\n",
    "    print(\"‚úÖ Soporta carga desde archivos (CSV, Excel, JSON)\")\n",
    "    print(\"‚úÖ Validaci√≥n y limpieza autom√°tica de datos\")\n",
    "    print(\"‚úÖ Tres tipos de pruebas t autom√°ticas\")\n",
    "    print(\"‚úÖ Visualizaciones y reportes completos\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ INICIO R√ÅPIDO:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nüìÅ Para analizar desde archivo:\")\n",
    "    print(\">>> resultado = analizar_desde_archivo('upload')\")\n",
    "\n",
    "    print(\"\\nüßÆ Para datos en memoria:\")\n",
    "    print(\">>> datos = crear_datos_ejemplo('simple')\")\n",
    "    print(\">>> resultado = analizar_t_test(datos, mu0=100)\")\n",
    "\n",
    "    print(\"\\nüìä Para datos con dos grupos:\")\n",
    "    print(\">>> datos = crear_datos_ejemplo('independientes')\")\n",
    "    print(\">>> resultado = analizar_t_test(datos)\")\n",
    "    print(\">>> # O para DataFrame con mismo tama√±o:\")\n",
    "    print(\">>> datos_df = crear_datos_ejemplo('independientes_df')\")\n",
    "    print(\">>> resultado = analizar_t_test(datos_df)\")\n",
    "\n",
    "    print(\"\\nüîÑ Para datos pareados:\")\n",
    "    print(\">>> datos = crear_datos_ejemplo('pareados')\")\n",
    "    print(\">>> resultado = analizar_t_test(datos, paired=True)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üõ†Ô∏è FUNCIONES AUXILIARES:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚Ä¢ mostrar_metodos_carga() - Info sobre carga de archivos\")\n",
    "    print(\"‚Ä¢ crear_datos_ejemplo() - Crear datos para practicar\")\n",
    "    print(\"  - tipos: 'simple', 'independientes' (tupla), 'independientes_df' (DataFrame)\")\n",
    "    print(\"           'pareados', 'problemas'\")\n",
    "\n",
    "    print(\"\\nüí° CONSEJOS IMPORTANTES:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚Ä¢ Para muestras INDEPENDIENTES con diferentes tama√±os:\")\n",
    "    print(\"  - El sistema detecta autom√°ticamente tama√±os diferentes\")\n",
    "    print(\"  - NO elimina filas autom√°ticamente\")\n",
    "    print(\"  - Cada grupo mantiene su tama√±o original\")\n",
    "    print(\"  - Usa prueba t de Welch por defecto para mayor robustez\")\n",
    "    print(\"‚Ä¢ Para muestras PAREADAS:\")\n",
    "    print(\"  - Necesita el mismo n√∫mero de observaciones\")\n",
    "    print(\"  - Use DataFrames con filas completas sin NaN intermedios\")\n",
    "    print(\"‚Ä¢ Para ARCHIVOS:\")\n",
    "    print(\"  - CSV/Excel: columnas separadas = muestras independientes\")\n",
    "    print(\"  - Filas completas = datos pareados\")\n",
    "    print(\"  - NaN al final de columnas = normal para independientes\")\n",
    "\n",
    "    print(\"\\nüîß SOLUCI√ìN DE PROBLEMAS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚Ä¢ Si ve 'datos pareados' cuando son independientes:\")\n",
    "    print(\"  - Verifique que los grupos tengan diferente tama√±o\")\n",
    "    print(\"  - Use tuplas: (grupo1, grupo2) para forzar independientes\")\n",
    "    print(\"‚Ä¢ Si los grupos se recortan al mismo tama√±o:\")\n",
    "    print(\"  - El sistema ahora preserva tama√±os originales\")\n",
    "    print(\"  - Los NaN al final de columnas no son problemas\")\n",
    "\n",
    "    print(\"\\nüéØ ¬°Listo para usar!\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
